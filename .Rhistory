aes(xintercept=grp.mean, color=Party), linetype="dashed")+
scale_x_continuous(limits = c(-2,2), name = "Party Positions") +
scale_y_continuous(name = "Density")+
ggtitle("Density plot of Party Positions in 2013")
de_13
prob_left <- cdu - afd1
view(prob_left)
prob_left <- c(cdu1 - afd1) - c(cdu-afd)
view(prob_left)
prob_left <- c(cdu1 - afd1) - c(afd-cdu)
view(prob_left)
prob_left <- c(cdu - afd1) - c(cdu1-afd)
view(prob_left)
mean(cdu1, cdu)
mean(cdu)
#calculate difference in distributions
mean(cdu <- ppois(qv,lambda = 0.8698601)) #lower tail
#calculate difference in distributions
mean(cdu <- ppois(qv,lambda = 0.8698601)) #lower tail
mean(cdu1 <- ppois(qv, lambda = 0.8698601), lower.tail = FALSE)#upper tail
mean(afd <- ppois(qv, lambda = 1.0768273))
mean(afd1 <- ppois(qv, lambda=1.0768273, lower.tail = FALSE))
prob_left <- c(cdu - afd1) - c(cdu1-afd)
view(prob_left)
#calculate difference in
prob_left <- ppois(0.8698601, lambda = 1.0768273))
#calculate difference in
prob_left <- ppois(0.8698601, lambda = 1.0768273)
view(prob_left)
#calculate difference in
prob_left <- ppois(0.8698601, lambda = 1.0768273, lower.tail = FALSE)#mean positon
view(prob_left)
#calculate difference in
prob_left <- ppois(0.8698601, lambda = 1.0768273, lower.tail = TRUE)#mean positon
view(prob_left)
#calculate difference in
prob_left <- ppois(0.8698601, lambda = 1.0768273, lower.tail = TRUE)#mean positon
view(prob_left)
#calculating the probability
head(mean_dens)
#calculating the probability
head(mean_dens)
#calculate the prob of AfD to be less or equal to CDU
prob_left <- ppois(0.8698601, lambda = 1.0768273, lower.tail = TRUE)#arg=mean CDU, mean Afd
view(prob_left)
head(prob_left)
knitr::opts_chunk$set(echo = TRUE, comment = "")
library(tidyverse)
library(ggrepel)
library(ca)
library(quanteda.textmodels)
theme_set(theme_minimal())
# Read in, ignoring the parse failures because they're on a
# variable we won't use
allcmp20 <- read_csv("~/Documents/University/Hertie/Courses/Year 2/Text as Data/assignment3/data/MPDataset_MPDS2020a.csv")
allcmp20$partyabbrev[allcmp20$partyname == "Pirates"] <- "Avast!"
cmp20 <- allcmp20 %>%
select(edate, countryname, partyname, partyabbrev,
total, voteper = pervote, uncoded = peruncod,
matches("per\\d\\d\\d$")) %>% # per%d%d%d% are subcategories, so we ignore them
mutate(edate = as.Date(edate, format = "%d/%m/%Y"),
eyear = lubridate::year(edate), # make a nice year just in case we want to filter with it
label = paste(partyabbrev, eyear, sep = ":"), # for graphing
across(starts_with("per"), function(x) round(total * (x/100))))
itemcodes <- read_csv("~/Documents/University/Hertie/Courses/Year 2/Text as Data/assignment3/data/itemcodes.csv")
head(itemcodes)
counts <- select(cmp20, starts_with("per"))
counts[1:3, 1:7] # quick peek at the top left corner
mat <- data.matrix(counts)
rownames(mat) <- cmp20$label
colnames(mat) <- itemcodes$name
oldest <- as.Date("1990-01-01")
de_mat <- mat[cmp20$edate > oldest & cmp20$countryname == "Germany",]
de_mat[1:3, 1:4]
de_rest <- filter(select(cmp20, -starts_with("per")),
edate > oldest,
cmp20$countryname == "Germany")
#model 1
mod1 <- ca(de_mat)
names(mod1)
head(mod1$rowcoord)
# pull out the estimated coordinates
theta <- mod1$rowcoord[,1]
beta <- mod1$colcoord[,1]
#look to check if model seem to make substantive sense
head(theta)
head(beta)
#enforce SDP as left of FDP
switcheroo <- ifelse(theta[3] < theta[4], 1, -1)
#multiply theta and beta by switcheroo
theta_mult <- theta*switcheroo
beta_mult <- beta*switcheroo
#check if the multiplication results in intepretable values
head(theta_mult)
head(beta_mult)
#construct data to plot mod1
#subset de_rest to only contain edate, country and party names
mod1_d <- bind_cols(de_rest$edate, de_rest$partyname, theta_mult)
colnames(mod1_d) <- c("Date","Party","Positions")
head(mod1_d)
#plot positions overtime
mod1_p <- mod1_d %>% ggplot(
aes(x = Date, y = Positions, col = Party)) +
geom_line() +
labs(x = "Election Years", y = "Positions")
ggtitle("Model 1:Party Positions Variation Through Elections")
mod1_p
#subset itemcodes to keep only -1 and 1
itemcodes_r <- itemcodes %>%
subset(rile.valence != 0)
#keep only rile positions in counts
counts_r<- counts %>%
select(itemcodes_r$code)
#generate dataset for model 2
mat_r <- data.matrix(counts_r)
#generate Germany substet
de_mat_r <- mat_r[cmp20$edate > oldest & cmp20$countryname == "Germany",]
#finaly some scaling for model 2, now with subset data containing Rile
mod2 <- ca(de_mat_r)
#again, pull out theta and beta and switch signs
theta2 <- mod2$rowcoord[,1]
beta2 <- mod2$colcoord[,1]
#magic switch
switcheroo2 <- ifelse(theta2[3] < theta2[4], 1, -1)
theta_r <- theta2 * switcheroo2
beta_r <- beta2 *switcheroo2
#view the theta and beta
head(theta_r)
head(beta_r)
theta_r <- tibble(theta_r)
mod2_d <- bind_cols(de_rest$edate, de_rest$partyname, theta_r, de_rest$voteper)
colnames(mod2_d) <- c("Date","Party","RiLe","Vote share")
head(mod2_d)
mod2_p <- mod2_d %>% ggplot(
aes(x = Date, y = RiLe, col = Party)) +
geom_line() +
labs(x = "Election Years", y = "Positions") +
ggtitle("Model 2: Party Positions Variation Through Election Using Valence")
mod2_p
# grab the estimates from the real data
origmod <- ca(de_mat)
theta_o <- origmod$rowcoord[,1]
beta_o <- origmod$colcoord[,1]
signflip <- ifelse(theta_o[3] > theta_o[4], -1, 1)
theta_o <- signflip*theta_o
beta_o <- signflip*beta_o
# how many bootstrap samples to use
B <- 500
# some constants to save typing
N <- nrow(de_mat)
V <- ncol(de_mat)
# how long each document is
doclens <- rowSums(de_mat)
# take the original counts and divide each element by
# it's row total
probs <- sweep(de_mat, 1, doclens, FUN = '/')
# make a place for each bootstrap data set to land in
boot_sample <- matrix(0, nrow = nrow(de_mat),
ncol = ncol(de_mat))
# this will hold all our beta estimates
betas <- matrix(0, nrow = V, ncol = B)
rownames(betas) <- itemcodes$name
# this will hold all our theta estimates
thetas <- matrix(0, nrow = N, ncol = B)
rownames(thetas) <- de_rest$label
# a function to create a new data set, fit a model, flip
# signs as necessary, extract theta and beta, and store them
boot_mat <- function(b)
{
# report every 100 iterations
if (b %% 100 == 0)
cat(".")
# resample a new data set
for (i in 1:N)
boot_sample[i,] <- rmultinom(1, prob = probs[i,],
size = doclens[i])
# fit model to this data set
res <- ca(boot_sample)
# extract the position estimates
theta_r <- res$rowcoord[,1]
beta_r <- res$colcoord[,1]
# if SPD > FDP, make a sign-flipping multiplier to identify
signflip <- ifelse(theta_r[3] > theta_r[4], -1, 1)
# record these values in the boostrap sample collection
# that lives outside the loop
thetas[,b] <<- signflip * theta_r
betas[,b] <<- signflip * beta_r
}
# run this to get B bootstrap estimates in thetas and betas
for (b in 1:B) boot_mat(b)
# summarise the estimates in each row
quants <- apply(thetas, 1, quantile,
probs = c(0.025, 0.975))
quants <- t(quants) # flip it over
names(quants) <- c("lwr", "upr") # and rename
head(quants)
#bind confidence intervals
mod3_d <- bind_cols(de_rest$edate, de_rest$partyname, mod1_d$Positions, quants)
colnames(mod3_d) <- c("Date","Party","Positions")
#plot model with confidence intervals
mod3_p <- mod1_d %>% ggplot(
aes(x = Date, y = Positions, col = Party)) +
geom_line() +
geom_ribbon(aes(ymin = quants[,1],
ymax = quants[,2], fill = Party), linetype=3, alpha=0.1) +
labs(x = "Election Years", y = "Positions - Valence") +
ggtitle("Party Positions Variation Through Elections",
subtitle = "95% Confidence Intervals in Shades")
mod3_p
#data about party positions in 2013
elec2013 <- thetas[de_rest$eyear == 2013, ]
party2013 <- de_rest$partyname[de_rest$eyear == 2013]
samps2013 <- data.frame(elec2013, party = party2013)
longsamps2013 <- pivot_longer(samps2013, -party)
colnames(longsamps2013) <- c("Party","x","Position")
head(longsamps2013)
#density plot for party positions in 2013
line <- "#1F3552"
#mean of position by party
mean_dens <- longsamps2013 %>%
group_by(Party) %>%
summarise(grp.mean=mean(Position))
#density plot of party positions in 2013
de_13 <- ggplot(
longsamps2013,aes(x=Position, group=Party, fill=Party)) +
geom_density(colour = line, alpha = 0.4) +
geom_vline(data=mean_dens,
aes(xintercept=grp.mean, color=Party), linetype="dashed")+
scale_x_continuous(limits = c(-2,2), name = "Party Positions") +
scale_y_continuous(name = "Density")+
ggtitle("Density plot of Party Positions in 2013")
de_13
#calculating the probability
head(mean_dens)
#calculate the prob of AfD to be less or equal to CDU
prob_left <- ppois(0.8698601, lambda = 1.0768273, lower.tail = TRUE)#arg=mean CDU, mean Afd
head(prob_left)
#centers of gravity analysis
gravity <- bind_cols(mod2_d$RiLe, ((de_rest$voteper)/100), de_rest$eyear)
names(gravity) <- c("Positions", "Percentage", "Year")
gravity <- gravity %>% mutate(wght.RiLe = Positions * Percentage)
ideology <- aggregate(. ~ Year, data=gravity, sum) %>%
select("Year", "wght.RiLe")
names(ideology) <- c("Year", "Position")
#plot Germany's position over time
#runs lm
p_de <- ideology %>% ggplot(aes(Year, Position))+
geom_line()+
ggtitle("German Ideology Over the Years, i.e. Elections")+
ylim(-2,2)
p_de
#plot positions overtime
mod1_p <- mod1_d %>% ggplot(
aes(x = Date, y = Positions, col = Party)) +
geom_line() +
labs(x = "Election Years", y = "Positions")+
ggtitle("Model 1:Party Positions Variation Through Elections")
mod1_p
library(httr) # for raw-ish http requests
library(jsonlite) # for json data parsing
parls <- GET("https://www.abgeordnetenwatch.de/api/v2/parliaments")
status_code(parls) # 200 is good https://www.flickr.com/photos/girliemac/sets/72157628409467125/ for reference
headers(parls)
View(parls)
status_code(parls) # 200 is good https://www.flickr.com/photos/girliemac/sets/72157628409467125/ for reference
headers(parls)
content(parls, "text")
p <- content(parls)
names(p)
p$data[[1]]$label_external_long
## processing lists (from json)
apply(p$data, function(x) x$label_external_long)
# sometimes we can safely switch to sapply (which will flatten lists)
sapply(p$data, function(x) x$label_external_long)
## paging
loons <- GET("https://www.abgeordnetenwatch.de/api/v2/candidacies-mandates?politician[entity.party.entity.short_name]=AFD")
# break out the endpoint
url <- "https://www.abgeordnetenwatch.de/api/v2/"
endpoint <- "candidacies-mandates"
q = list("politician[entity.party.entity.short_name]" = "AfD")
loons <- GET(paste0(url, endpoint), query = q)
lns <- content(loons)
first_hundred <- sapply(lns$data,
function(x) x$fraction_membership[[1]]$fraction$label)
# how to get 100 more?
q = list("politician[entity.party.entity.short_name]" = "AfD",
page = 2)
more_loons <- GET(paste0(url, endpoint), query = q)
sapply(lns$data, function(x) c(x$label, x$politician$id))
q = list(related_data = "show_information")
steffen <- GET(paste0(url, "politicians/", 145719), query = q)
content(steffen) # thanks for the instructions, aw!
file.edit("~/.Renviron")
Sys.getenv("TWFY_API_KEY")
q <- list(key = Sys.getenv("TWFY_API_KEY"),
output = "js",
name = "Keighley")
url <- "https://www.theyworkforyou.com/api/getConstituency"
keighley <- GET(url, query = q)
kk <- content(keighley)
names(p$meta)
grab_name <- function(x) {x$label_external_long}
grab_name(p$data[[1]])
grab_name(p$data[[4]])
kll <- lapply(p$data, grab_name)#attach the name from api to each one in same list
#unlist
unslit(kll)
#unlist
unlist(kll)
Corp_HouseOfCommons_V2 <- readRDS("~/Desktop/FP_textasdata/Corp_HouseOfCommons_V2.rds")
#load packages
library(readr)
library(quanteda)
library(readtext)
library(dplyr)
library(ggplot2)
#dictionary for sentiment analysis
#https://www.rdocumentation.org/packages/quanteda/versions/2.1.2/topics/data_dictionary_LSD2015
#load dictionary
dict_img <- read_csv("immigration_500.csv")
dict_img <- dict_img %>%
subset(select = -c(`100.0`))
#convert dictionary df to a list
dict <- split(dict_img, seq(nrow(dict_img)))
#load data
Corp_HouseOfCommons_V2  <- readRDS("~/Desktop/FP_textasdata/Corp_HouseOfCommons_V2.rds")
head(Corp_HouseOfCommons_V2$date)
view(Corp_HouseOfCommons_V2$date)
speeches <- Corp_HouseOfCommons_V2 %>%
select(!c(iso3country, party.facts.id, parliament) & date>"1999-01-01")
View(Corp_HouseOfCommons_V2)
speeches <- Corp_HouseOfCommons_V2 %>%
select(!c(iso3country, party.facts.id, parliament))
speeches_1 <- speeches %>%
filter(date>"1999-12-21")
View(speeches_1)
#turn into corpus
speechcorp <- corpus(speeches_1)
#summary
summary(speechcorp)
#dfm creates a document feature matrix
img_dfm <- dfm(speechcorp, dictionary = dict)
#load packages
library(readr)
library(quanteda)
library(quanteda.textmodels)
library(readtext)
library(tidyverse)
library(stm)
#load data
Corp_HouseOfCommons_V2  <- readRDS("~/Desktop/FP_textasdata/Corp_HouseOfCommons_V2.rds")
View(Corp_HouseOfCommons_V2)
install.packages(c("data.table", "dplyr", "ggplot2", "lme4", "lubridate", "quanteda", "quanteda.textmodels", "RColorBrewer", "readr", "readtext", "stm", "tidyverse"))
install.packages(c("caret", "dplyr", "expss", "haven", "tidyverse"))
install.packages("summarytools")
setwd("~/Desktop/Master thesis/MLhetrogeneouseffects")
#load packages
library(haven)
library(caret)
library(tidyverse)
library(dplyr)
library(expss)
library(summarytools)
ud <- read_dta("Uganda ELA Panel wide_Creation.dta")
# Step 1: understand how many na in each row (observarion)
obs_na <- apply(ud, MARGIN = 1, function(x) sum(is.na(x)))
obs_na
# Step 2: understand how many na in each column (variable) + descriptive
# Function for summarystats
custom_glimpse <- function(df_summary) {
descriptive <- data.frame(
col_name = colnames(df_summary),
col_index = 1:ncol(df_summary),
col_class = sapply(df_summary), # add class= c(double, numeric)?
col_mean = sapply(df_summary, mean),
col_obs = sapply(df_summary, count),
row.names = NULL,
return(descriptive)
)
}
custom_glimpse(ud)
?summarytools
dfSummary(ud)
print(dfSummary(ud), method = "render")
# Step 2: understand how many na in each column (variable) + descriptive
# descriptive stats
descriptive <- dfSummary(ud)
View(descriptive)
view(descriptive)
install.packages(c("caret", "expss"))
remove = c("immigra*","Immigra*","refugee*","Refugee*","asylum","Asylum"," migra*"," Migra*"),
remove_punct = TRUE,
remove = stopwords(),
remove_symbols = TRUE,
remove_separators = TRUE,
split_hyphens = TRUE,
remove_numbers = TRUE)
?remove
kwic_dfm_no_key <- dfm(corp_kwic,
remove = list(character="immigra*","Immigra*","refugee*","Refugee*","asylum","Asylum"," migra*"," Migra*"),
remove_punct = TRUE,
remove = stopwords(),
remove_symbols = TRUE,
remove_separators = TRUE,
split_hyphens = TRUE,
remove_numbers = TRUE)
#load packages
library(readr)
library(quanteda)
library(quanteda.textmodels)
library(readtext)
library(tidyverse)
library(data.table)
library(stm)
library(dplyr)
library(ggplot2)
library(lubridate)
library(lme4)
library(lattice)
kwic_dfm_no_key <- dfm(corp_kwic,
remove = list(character="immigra*","Immigra*","refugee*","Refugee*","asylum","Asylum"," migra*"," Migra*"),
remove_punct = TRUE,
remove = stopwords(),
remove_symbols = TRUE,
remove_separators = TRUE,
split_hyphens = TRUE,
remove_numbers = TRUE)
kwic_dfm_no_key <- dfm(corp_kwic,
remove = list(character="immigra*","Immigra*","refugee*","Refugee*","asylum","Asylum"," migra*"," Migra*"),
remove_punct = TRUE,
remove = stopwords(),
remove_symbols = TRUE,
remove_separators = TRUE,
split_hyphens = TRUE,
remove_numbers = TRUE)
library(stm)
library(readr)
library(quanteda)
library(quanteda.textmodels)
library(readtext)
library(tidyverse)
library(data.table)
library(stm)
library(dplyr)
library(ggplot2)
library(lubridate)
library(lme4)
library(lattice)
install.packages("data.table")
library(readr)
library(quanteda)
library(quanteda.textmodels)
library(readtext)
library(tidyverse)
library(data.table)
library(stm)
library(dplyr)
library(ggplot2)
library(lubridate)
library(lme4)
library(lattice)
install.packages("data.table")
install.packages("stm")
kwic_dfm_no_key <- dfm(corp_kwic,
remove = list(character="immigra*","Immigra*","refugee*","Refugee*","asylum","Asylum"," migra*"," Migra*"),
remove_punct = TRUE,
remove = stopwords(),
remove_symbols = TRUE,
remove_separators = TRUE,
split_hyphens = TRUE,
remove_numbers = TRUE)
install.packages(c("codetools", "KernSmooth", "nlme"))
kwic_dfm_no_key <- dfm(corp_kwic,
remove = list(character="immigra*","Immigra*","refugee*","Refugee*","asylum","Asylum"," migra*"," Migra*"),
remove_punct = TRUE,
remove = stopwords(),
remove_symbols = TRUE,
remove_separators = TRUE,
split_hyphens = TRUE,
remove_numbers = TRUE)
kwic_dfm_no_key <- dfm(corp_kwic,
remove_punct = TRUE,
remove = stopwords(),
remove_symbols = TRUE,
remove_separators = TRUE,
split_hyphens = TRUE,
remove_numbers = TRUE)
setwd("~/Desktop/tada-hoc")
Corp_HouseOfCommons_V2  <- readRDS("~/Desktop/Corp_HouseOfCommons_V2.rds")
#year: subset to speeches from 2010 (Justification:Tory manifesto)
speeches <- Corp_HouseOfCommons_V2 %>%
select(!c(iso3country, party.facts.id, parliament)) %>%
filter(date>"2009-12-20")
toMatch <- c("immigra*","Immigra*","refugee*","Refugee*","asylum","Asylum"," migra*"," Migra*")
agenda_text_filter <- filter(speeches, grepl(paste(toMatch,collapse="|"), agenda) | grepl(paste(toMatch,collapse="|"), text))
install.packages("mice")
speechcorp <- corpus(agenda_text_filter)
install.packages("stm")
library(readr)
library(quanteda)
library(quanteda.textmodels)
library(readtext)
library(tidyverse)
library(data.table)
library(stm)
library(dplyr)
library(ggplot2)
library(lubridate)
library(lme4)
library(lattice)
install.packages("data.table")
speechcorp <- corpus(agenda_text_filter)
